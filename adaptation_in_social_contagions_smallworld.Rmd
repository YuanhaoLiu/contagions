---
title: "adaptation_in_contagions"
author: "Colin Yuanhao Liu"
date: "11/29/2021"
output: html_document
---

```{r message=FALSE, warning=FALSE}
rm(list=ls());gc()
#library(ggplot2)
#library(tidyverse)
#library(dbplyr)
library(igraph)
library(data.table)
```


```{r message=FALSE, warning=FALSE}
#small world network
SM_network = function(n=45, #number of agents
                      m=5,
                      k=6, #number of preference dimensions
                      d=.95, #decay parameter (lower D = faster decay; vary in range 0-1)
                      iters = 1000000, #number of ticks the model will run for
                      measurement_interval = 10000, #how many ticks between population measurements
                      From_Core = T,
                      SmallWorld = T ) #T = from core, F = from periphery
  
{
  #Assign agent attributes:
  #1 - Preferences
  #Create data frame with one row per agent and one column/variable per preference
  #Each cell will be given a random value between -1 and 1  
  p1 = data.frame(matrix(runif((n)*k, min= 0, max=2), nrow = n, ncol = k))
  p2 = data.frame(matrix(runif((m)*k, min= -2, max=0), nrow = m, ncol = k))
  p = rbind(p1,p2)
  #2 - Associations
  #Create an empty list
  a = list()
  
  #For each agent, we create a K x K matrix (K = number of preference dimensions) 
  #Each cell initially has the value 1 in it
  #Add each matrix to the list 
  mat = matrix(1, nrow = k, ncol = k)
  for(y in 1:(n+m)) {
    a[[y]] = mat
  }
  
  #Create data frame to store results
  tick = seq(1:iters)
  results = data.frame(tick) #store results here
  results$pref_sim=NA #preference similarity (see Goldberg & Stein for explanations of measures)
  results$pref_cong=NA #preference congruence 
  
  #Create network topology
  if(From_Core == T) {
    g = sample_smallworld(1, (n+m), 5, 0.1)
    g$bet = betweenness(g)
    DF1 <- as.data.frame(get.edgelist(g))
    DF2 <- data.frame(V1 = as.vector(V(g)), bet=g$bet)
    V <- order(DF2$bet)
    DF3 <- data.frame(DF2[V, ], rank = as.numeric(factor(DF2$bet[V])))
    setDT(DF1)
    setDT(DF3)
    DF1[DF3, on = c("V1"), V1 := i.rank]
    colnames(DF3)[1] <-"V2"
    DF1[DF3, on = c("V2"), V2 := i.rank]
    DF4 <- DF1[order(-V1, -V2),]
    #rownames(DF4) <- 1:nrow(DF4)
    net = graph.data.frame(DF4, directed = F)
    net = simplify(net)
    net = delete_vertices(net, degree(net)==0)
  } else{
    g = sample_smallworld(1, (n+m), 5, 0.1)
    g$bet = betweenness(g)
    DF1 <- as.data.frame(get.edgelist(g))
    DF2 <- data.frame(V1 = as.vector(V(g)), bet=g$bet)
    V <- order(DF2$bet)
    DF3 <- data.frame(DF2[V, ], rank = as.numeric(factor(DF2$bet[V])))
    setDT(DF1)
    setDT(DF3)
    DF1[DF3, on = c("V1"), V1 := i.rank]
    colnames(DF3)[1] <-"V2"
    DF1[DF3, on = c("V2"), V2 := i.rank]
    #rownames(DF4) <- 1:nrow(DF4)
    DF4 <- DF1[order(V1, V2),]
    net = graph.data.frame(DF4, directed = F)
    net = simplify(net)
    net = delete_vertices(net, degree(net)==0)
  }
  
  #Update agents
  for(i in 1:iters) {
    
    #1 - An actor and observer (so 2 agents total) are randomly selected to interact
    if(SmallWorld == T) {
      #small world topology 
      observer = sample(1:(n+m), 1) 
      actor = sample(adjacent_vertices(net, observer, mode=c("out"))[[1]], 1)
    } else{ #Fully connected
      agents = sample(1:(n+m), 2) #First is actor, Second is observer 
      actor = agents[1]
      observer = agents[2]
    }
    
    #2- The actor (Agent 1) enacts two practices (based on her preferences)
    behaviors = sample(1:k, #1 through K 
                       2, #pick two preferences
                       prob = (exp( p[actor, ])/( sum(exp(p[actor, ])))))
    
    #3 - The observer (Agent 2) updates perception of associations between these practices
    a[[observer]][behaviors[1], behaviors[2]] = a[[observer]][behaviors[1], behaviors[2]] + 1
    a[[observer]][behaviors[2], behaviors[1]] = a[[observer]][behaviors[2], behaviors[1]] + 1 
    
    #4 - Observer (Agent 2) updates one preference (the weaker of the two)
    #"Weaker" here means that the absolute value of the distance between observer's preference and the mean preference is lower
    updated_preference = ifelse((abs(p[observer, behaviors[1]] - rowMeans(p[observer,]))) < (abs(p[observer, behaviors[2]] - rowMeans(p[observer,]))), 
                                behaviors[1], #return weaker of two preferenceds
                                behaviors[2])
    
    #5 - Observer updates selected preference from step 4 
    old_prefs = p[observer,]
    
    new_prefs = old_prefs
    #new_prefs[updated_preference] = old_prefs[updated_preference] + runif(1, min = -1, max = 1)
    #update using a normal instead of uniform distribution
    #new_prefs[updated_preference] = old_prefs[updated_preference] + rnorm(1, mean = 0, sd = 1)
    new_prefs[updated_preference] = old_prefs[updated_preference] + runif(1, min = -2, max = 2)
    #6 - Test constraint satisfaction
    
    #convert observer's preference vector into matrix of differences
    old_pref_matrix = abs(outer(as.numeric(old_prefs), as.numeric(old_prefs), FUN="-"))
    new_pref_matrix = abs(outer(as.numeric(new_prefs), as.numeric(new_prefs), FUN="-"))
    
    #standarize all three relevant matrices (old pref, new pref, and associations) by maximum values
    old_pref_matrix = old_pref_matrix/max(old_pref_matrix)
    new_pref_matrix = new_pref_matrix/max(new_pref_matrix)
    
    assoc_std = a[[observer]]/max(a[[observer]])
    
    #Calculate CS for old and new preferences
    old_CS = (1 / (k * (k - 1))) * sum(abs(assoc_std - old_pref_matrix))
    new_CS = (1 / (k * (k - 1))) * sum(abs(assoc_std - new_pref_matrix)) 
    
    #If CS with new preference is higher: Keep NEW preference
    #Otherwise: Keep OLD preference
    if(new_CS > old_CS) {
      p[observer,]=new_prefs
    }
    
    #7 - Apply decay function to associations
    a[[observer]] = a[[observer]] * d
    
    #Record population-level outcomes
    if( (i/measurement_interval) == round(i/measurement_interval) | i == 1) {
      print(i)
      
      pref_sim_mat = matrix(NA, nrow=(n+m), ncol=(n+m))
      pref_mean = matrix(NA, nrow=(n+m), ncol=(n+m))
      for(x in 1:(n+m)) {
        for(y in 1:(n+m)) {
          pref_sim_mat[x, y]=cor(as.numeric(p[x,]), as.numeric(p[y,]))
          pref_mean[x, y]= mean((as.numeric(unlist(p[x,]))+as.numeric(unlist(p[y,])))/2)
        }
      }
      
      #pref_mean_new = matrix(NA, nrow=(n+m), ncol=k)
      #for (a in 1:(n+m)){
      #  pre_mean[a] = mean(as.numeric(p[x,]))
      #}
      
      pref_sim_mat[lower.tri(pref_sim_mat)] = NA #preference similarity only measured for unique pairs
      diag(pref_sim_mat)=NA #set diagonals to not count

      
      results$pref_sim[i] = mean(pref_sim_mat, na.rm=T)
      results$pref_cong[i] = mean(abs(pref_sim_mat), na.rm=T)  
      results$pref_mean[i] = mean(pref_mean, na.rm = T)
      #restuls$pref_mean_new[i] = mean(pref_mean_new, na.rm=T)
      
      #Stop if model has converged
      if(results$pref_cong[i] >= .99) {
        break
      }
    }
  }
  
  #Clean and return results data frame
  results = subset(results, !(is.na(results$pref_cong)) )
  return(results)
}

```


```{r}
results1=SM_network(n=45, m = 5, d=.95, iters = 1000000, measurement_interval = 10000, From_Core = T, SmallWorld = T)
write.csv(results1, "sm_c2p_180_20.csv")

results2=SM_network(n=45, m = 5, d=.95, iters = 1000000, measurement_interval = 10000, From_Core = F, SmallWorld = T)
write.csv(results2, "sm_p2c_180_20.csv")
```


```{r message=FALSE, warning=FALSE}
#small world network 10% actors accept a correction 
ggplot(results1, aes(x=tick))+ 
  (aes(y=pref_mean, colours = "hp")) +
  geom_point()
```
